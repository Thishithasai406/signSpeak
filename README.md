# âœ‹ ASL Sign Language to Text and Speech Conversion

This project performs **real-time American Sign Language (ASL) alphabet recognition** (Aâ€“Z) and converts hand signs into **text** and optionally **speech**.

![website](SignSpeak.png)
It uses:
- **Mediapipe** (via CVZone) to track the userâ€™s hand in real-time.
- **A CNN Model** (`cnn8grps_rad1_model.h5`) trained to predict one of **8 gesture groups**.
- **Rule-based logic** to refine group predictions into **26 ASL letters**.
- **Spell checking** + **Text-to-Speech** for enhanced communication.

ğŸ“Œ The model is trained on the **ASL Mediapipe Landmarked Dataset (Aâ€“Z)** available on Kaggle:

> https://www.kaggle.com/datasets/granthgaurav/asl-mediapipe-converted-dataset

In this project, the CNN model (`cnn8grps_rad1_model.h5`) was trained using this dataset and the
trained weights are saved in the repository so you can run realâ€‘time inference without reâ€‘training.

---

## ğŸ”¤ ASL Alphabet Reference

(![ASL Alphabet A-Z](sign.png)

---

## â“Why Alphabet-Level Recognition Instead of Whole Words

This project focuses on recognizing individual ASL alphabet letters (Aâ€“Z) rather than full ASL words. The main reasons are:
| Benefit | Description |
|--------|-------------|
| Beginner-friendly | You only need to learn 26 hand signs |
| More universal | Fingerspelling is widely used in ASL |
| Any word can be formed | Even unseen/new vocabulary |
| Less data required | Full-word ASL recognition requires huge datasets |


While alphabet-based communication is slower than using full ASL vocabulary, it is the most accessible, universal, and scalable entry point for real-time sign-language interpretation.

---

## ğŸš€ Features

âœ” Real-time ASL alphabet recognition  
âœ” 21-point hand landmark tracking  
âœ” Skeleton rendering input to CNN  
âœ” CNN predicts gesture groups (0â€“7)  
âœ” Rule-based refinement to A-Z  
âœ” Sentence construction using gestures  
âœ” Spell suggestions using `pyenchant`  
âœ” Text-to-speech output using `pyttsx3`  
âœ” Simple and user-friendly Tkinter GUI  

---

## 1. ğŸ“š Dataset

- **Name**: ASL Mediapipe Landmarked Dataset (Aâ€“Z)
- **Source**: Kaggle â€“ `granthgaurav/asl-mediapipe-converted-dataset`
- **Classes**: 26 letters (Aâ€“Z) of American Sign Language fingerspelling.
- **Samples per class**: 180 images per letter.
- **Total images**: 4,680+ images (4681 in my split: 3,276 train, 702 validation, 703 test).
- **Data format**:
  - Images where the hands have already been processed with **Mediapipe**.
  - Each image encodes the **hand landmarks** (finger joint locations and orientation) as a rendered skeleton.
  - Mediapipe provides **x, y, z** coordinates for each joint â†’ converted into clean **white-background skeleton images** used for training.

### ğŸ¯Why We Used Skeleton Dataset Instead of Bare Hand Images

We used a Mediapipe-landmark skeleton dataset instead of raw hand camera images because:

- **Background Independent**  
  Only the hand pose is captured â€” no background, clothing, or lighting distractions.

- **Skin Tone Independent**  
  Since only joints and structure are used, performance does not depend on skin color.

- **Higher Accuracy with Less Data**  
  Skeleton-based learning focuses purely on geometry, so **~4680 images** are enough.  
  Raw images would require **50,000+ training samples** to achieve similar results.

- **Robust in Real-Time**  
  The model works smoothly across different environments and camera quality.

- **Geometric Rule Integration**  
  Exact joint locations allow **rule-based refinement** of look-alike letters  
  (e.g., M/N/S/T, U/V/W, G/H, P/Q/Z), which is not possible with raw images alone.

- **Fast Training & Inference**  
  Smaller, meaningful input â†’ quicker prediction â†’ real-time performance.

> Skeleton dataset provides only the essential hand structure,
> making the model more **accurate, efficient, and environment-independent** 
> than training with bare hand images.


### âœ‹Mediapipe Hand Landmarks

![Hand Landmarks](handlandmark.png)

The Mediapipe framework detects and tracks **21 hand landmarks** on each hand. These landmarks represent key anatomical points:

- **Landmark 0**: Wrist (base of hand)
- **Landmarks 1â€“4**: Thumb (CMC, MCP, IP, Tip)
- **Landmarks 5â€“8**: Index finger (MCP, PIP, DIP, Tip)
- **Landmarks 9â€“12**: Middle finger (MCP, PIP, DIP, Tip)
- **Landmarks 13â€“16**: Ring finger (MCP, PIP, DIP, Tip)
- **Landmarks 17â€“20**: Pinky finger (MCP, PIP, DIP, Tip)

Each landmark is represented by normalized `(x, y, z)` coordinates, where:
- **x, y**: 2D position in the image (0â€“1 range, relative to frame dimensions)
- **z**: Depth information (approximate distance from camera)

These 21 landmarks are used to:
1. **Render the skeleton image** (green lines connecting joints, red circles at landmarks)
2. **Extract geometric features** for rule-based refinement (distances, angles, relative positions)
3. **Make the system robust** to background, lighting, and camera variations

---

## 2. ğŸ§  Model Training

The system is split into two main parts:

1. **Offline training** (on the Kaggle dataset)
2. **Online inference application** (`prediction.py`) that runs on a webcam stream


### ğŸ”¹ Dataset
- Used ASL Mediapipe Skeleton Dataset from Kaggle
- Each sample already contains **21 hand landmarks** converted into a **skeleton image**
- Letters grouped into **8 similar-gesture categories** for better accuracy

### ğŸ”¹ Pre-processing
- All images resized to **400 Ã— 400 Ã— 3**
- Pixel values normalized to **0â€“1**
- Data split into:
  - 80% â†’ Training Set
  - 20% â†’ Validation Set

### ğŸ”¹ Model Architecture
- Convolutional Neural Network (CNN) built using **TensorFlow + Keras**
- Main layers include:
  - Convolution Layers (feature extraction)
  - Max-Pooling Layers (down-sampling)
  - Dense Layers (classification)
- Final layer uses **Softmax** activation  
  â†’ Predicts **8 gesture groups**

### ğŸ”¹ Output
- Trained model saved as:  
  **`cnn8grps_rad1_model.h5`**
- This model is later used during real-time prediction to classify gesture groups

---

## 3. ğŸ§© Why this Method

### 3.1. Why Mediapipe Landmarks + Skeleton Images

- **Robustness to background**: By working on landmark skeletons instead of raw images, the model ignores
  background clutter, clothing, and lighting variations.
- **Geometric interpretability**: Landmark coordinates correspond to real anatomical points (finger joints). That
  allows building meaningful **ruleâ€‘based constraints** on top of the CNN.
- **Data efficiency**: Landmarks compress the visual information into a lowâ€‘dimensional pose; this typically
  requires fewer images to generalize well.

### 3.2. Why an 8â€‘Group CNN instead of 26 Direct Classes

Certain ASL letters have very similar hand shapes.  
Example letter look-alike groups:

- **A / E / M / N / S / T**
- **U / V / W**
- **G / H**
- **P / Q / Z**
- **I / J**
- **B / D / F / R / K**
- **C / O**

If we train the model to directly classify **26 letters**, the CNN easily gets confused within these similar shapes.

So instead, we convert the 26 letters into **8 larger gesture groups**:
| Group | Letters in Group |
|-------|-----------------|
| 0 | A, E, M, N, S, T |
| 1 | B, D, F, I, U, V, W, K, R |
| 2 | C, O |
| 3 | G, H |
| 4 | L |
| 5 | P, Q, Z |
| 6 | X |
| 7 | Y, J |

This gives multiple benefits:

#### âœ” Higher Accuracy
Model only needs to separate **8 groups** â†’ easier learning â†’ better performance.

#### âœ” Less Training Data Required
26-class training needs **much larger dataset**.  
Grouping allows strong results with smaller dataset (~4680 images).

#### âœ” Reduces Misclassification of Look-alike Letters
Fine separation is done later using **geometric rules** on Mediapipe landmarks.

#### âœ” Faster & More Stable Training
Less output complexity â†’ faster training & real-time prediction stability.

ğŸ‘‰ Final Refinement:
Once the model predicts the correct **group**,  
rule-based logic uses distance/angle checks to determine the **exact letter**.

> Combining CNN (for general shape) + Rules (for fine differences)  
> gives the best performance with limited data and maximum accuracy.


This hybrid approach takes advantage of both deep learning and classical ruleâ€‘based logic.

### 3.3. Why CNN for Training

- CNNs are the **standard choice for imageâ€‘based classification** and work very well on structured inputs like the
  400Ã—400 skeleton images.
- Convolutions capture local patterns such as fingertip arrangements and palm orientation.
- The preprocessed images are small enough that a moderateâ€‘sized CNN can be trained efficiently even on
  midâ€‘range GPUs or CPUs.

---

## 4. âš™ï¸Training Setup 


- **Input**: 400Ã—400Ã—3 images of Mediapipeâ€‘rendered hand skeletons.
- **Target**: One of 8 group labels (0â€“7) as described above.
- **Model type**: Keras CNN with a softmax output layer over 8 classes.
- **Loss**: Crossâ€‘entropy classification loss.
- **Metrics**: Topâ€‘1 accuracy, Topâ€‘3 accuracy, and crossâ€‘entropy.
- **Data split** (groupâ€‘level):
  - Train: 3,276 images
  - Validation: 702 images
  - Test: 703 images

### 4.1. Evaluation Results (Groupâ€‘Level)

![Evaluation](Evaluation.png)

---

## 5. ğŸ—‚ï¸ Project Structure


- `prediction.py` â€“ Main application  
- `cnn8grps_rad1_model.h5` â€“ Trained CNN model  
- `dataset/` â€“ Training dataset  
- `requirements.txt` â€“ Dependencies  

---

## 6. ğŸƒ How to Run the Application

### 6.1. Prerequisites

- **Python**: 3.x
- **Operating System**: Tested on Windows (other OSes may also work with the right dependencies).
- A working **webcam**.

### 6.2. Install Dependencies

From the project root directory:

```bash
pip install -r requirements.txt
```

Make sure you have the following key packages (they are listed in `requirements.txt`):

- `opencv-python`
- `cvzone`
- `mediapipe` (used internally by cvzone)
- `tensorflow` / `tensorflow-gpu` (for loading and running the CNN)
- `pyttsx3` (textâ€‘toâ€‘speech)
- `pyenchant` (spelling suggestions)
- `Pillow` (image handling for Tkinter)

### 6.3. Run

From the project root:

```bash
python prediction.py
```

This will:

- Open a GUI window.
- Start the webcam feed on the left side.
- Continuously detect your hand and display the recognized character and sentence on the right.
- Offer spelling suggestions below the camera view.
- Allow you to **Clear** the sentence or **Speak** it via textâ€‘toâ€‘speech.


### 6.4. Notes on Gestures and Special Controls

- Certain poses are mapped to control tokens:
  - **`next`** gesture: Confirms the current character and appends it to the sentence.
  - **`Backspace`** gesture: Deletes the last character.
  - **`Double spaces or pause gestures`** insert space characters between words.
- A temporal buffer of the last 10 predictions is used to avoid accidental triggers from noisy frames.

These rules are implemented in the `predict` method in `prediction.py`.

---

## ğŸ§‘â€ğŸ’» User Flow â€“ How the System Works

1ï¸âƒ£ **User shows a hand gesture** in front of the webcam.  
The camera captures the live hand image.

2ï¸âƒ£ **Mediapipe Hand Tracking** detects the hand and extracts  
21 key-points (x,y,z coordinates) of the hand skeleton.

3ï¸âƒ£ **CVZone Visualization** draws the skeleton on a clean  
white background for better feature clarity.

4ï¸âƒ£ The 21 landmark points are sent to the **trained CNN model**  
(TensorFlow) which predicts the correct **gesture group (0â€“7)**.

5ï¸âƒ£ **Rule-Based Decision Logic** checks specific finger angles and distances  
to identify the **exact letter (Aâ€“Z)** within that group.

6ï¸âƒ£ The **predicted letter** is shown on the screen in real-time.

7ï¸âƒ£ **Auto-Suggestions Feature**  
The last typed word is checked using a dictionary  
and **word suggestions** are displayed for correction.

8ï¸âƒ£ **Clear Button**  
User can clear the entire text with one click.

9ï¸âƒ£ **Speak Button**  
pyttsx3 converts the written sentence to speech  
allowing the system to **speak the predicted text** out loud.

ğŸ“Œ Summary:  
Camera â†’ Mediapipe (features) â†’ CNN Model (Group) â†’ Rules (Final Letter)

---

## 7. ğŸŒŸ Possible Extensions

- **Reâ€‘training or fineâ€‘tuning** on:
  - More users and lighting conditions.
  - Dynamic signs (short sequences) instead of single letters.
- **Direct landmark model**:
  - Train a network directly on 3D/2D landmark coordinates instead of their rendered images.
- **Export to mobile / web** using TensorFlow Lite or WebAssembly.
- **Multi-language support**: Add support for other sign language systems such as ISL, BSL, etc.
---

## 8. ğŸ™Œ Acknowledgements

- **Dataset**: [ASL Mediapipe Landmarked Dataset (Aâ€“Z)](https://www.kaggle.com/datasets/granthgaurav/asl-mediapipe-converted-dataset)
  by Granth Gaurav.
- **Hand tracking and Landmark Detection**: Mediapipe and cvzone.
- **Deep learning framework**: TensorFlow with Keras.
- **Textâ€‘toâ€‘speech**: pyttsx3.
- **Spell checking**: pyenchant.

- **Computer Vision Processing**: OpenCV (for image capture, processing, and display)


